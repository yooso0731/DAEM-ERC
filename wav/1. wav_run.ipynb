{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f74b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torchmetrics import F1Score, ConfusionMatrix\n",
    "from torchsummary import summary\n",
    "\n",
    "from wav_preprocess import *\n",
    "from wav_classifier import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# evaluation functions\n",
    "def valid_model(model, dataset, batch_size, loss_func, device=device):\n",
    "  model.eval()\n",
    "  dl = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  f1_func = F1Score(task='multiclass', num_classes=7).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_outputs = torch.empty(0)\n",
    "    for xb, yb in dl:\n",
    "      xb, yb = xb.to(device), yb.to(device)\n",
    "      outputs = model(xb)\n",
    "      val_loss += loss_func(outputs, yb).item() / len(dl)\n",
    "\n",
    "      val_f1 += f1_func(outputs, yb) / len(dl)\n",
    "\n",
    "    return val_loss, val_f1.item()\n",
    "\n",
    "def test_model(model, x, y, batch_size, device=device):\n",
    "  model.eval()\n",
    "  dl = DataLoader(TensorDataset(x, y), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    val_f1 = 0\n",
    "    val_outputs = torch.empty(0).to(device)\n",
    "    for xb, yb in dl:\n",
    "      xb, yb = xb.to(device), yb.to(device)\n",
    "      outputs = model(xb)\n",
    "      val_outputs = torch.cat((val_outputs, outputs), 0)\n",
    "\n",
    "    return val_outputs\n",
    "\n",
    "\n",
    "# 모델 결과 저장 및 성능 확인 \n",
    "def save_modelFile(path, x, y, model, annotation, batch_size, fold='train', device=device):\n",
    "  f1_func = F1Score(task='multiclass', num_classes=7).to(device)\n",
    "  confmat = ConfusionMatrix(task='multiclass', num_classes=7).to(device)\n",
    "\n",
    "  model.eval()\n",
    "  all_outputs = torch.empty(0).to(device)\n",
    "  with torch.no_grad():\n",
    "    dl = DataLoader(TensorDataset(x, y), batch_size=batch_size, shuffle=False)\n",
    "    for xb, yb in tqdm(dl):\n",
    "      xb, yb = xb.to(device), yb.to(device)\n",
    "      outputs = model(xb)\n",
    "      all_outputs = torch.cat((all_outputs, outputs))\n",
    "\n",
    "  f1_score = f1_func(all_outputs, y.to(device))\n",
    "  conf_mat = confmat(all_outputs.to(device), y.to(device))\n",
    "\n",
    "  # save\n",
    "  anno = annotation.segment_id.tolist()\n",
    "  if path:\n",
    "      outputs_csv = pd.DataFrame(all_outputs.cpu().numpy(), columns=['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'])\n",
    "      outputs_csv['segment_id'] = anno\n",
    "      outputs_csv = outputs_csv[['segment_id', 'angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']]\n",
    "      outputs_csv.to_pickle(path+'.pkl')\n",
    "  \n",
    "  return f1_score, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b6639",
   "metadata": {},
   "source": [
    "### 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffa99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "######### Parameters ##########\n",
    "# annotation: 학습 데이터 셋의 session_id에 따른 감정 레이블 확인 데이터 -- 공통 전처리 파일 이후 저장된 데이터임\n",
    "# aug_dict: 감정 클래스 id별 증강 배율 \n",
    "# wav_PATH: directory of audio files (~~/wav/session/...)\n",
    "# save_PATH: directory to save (augment dataset)\n",
    "###############################\n",
    "\n",
    "aug_dict = {0: 90, 1: 200, 2: 260, 3: 10, 4: 1, 5: 100, 6: 90}  # (neutral 클래스에 맞게 증강할 경우 대비)\n",
    "PATH = '../dataset/KEMDy20_v1_1'\n",
    "save_PATH = '../dataset/KEMDy20_v1_1/new/wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e73ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14612, 1, 40, 251]) torch.Size([14612])\n"
     ]
    }
   ],
   "source": [
    "annotation = pd.read_pickle(PATH+'/new/annotation/all_annotation.pkl')\n",
    "\n",
    "dl = data_load(annotation, data_path=PATH+'/wav')\n",
    "mfcc_train_aug_x, mfcc_train_aug_y = dl.get_data(method='mfcc', fold='train', aug=True, aug_dict=aug_dict)\n",
    "mfcc_train_x, mfcc_train_y = dl.get_data(method='mfcc', fold='train', aug=False)\n",
    "\n",
    "print(mfcc_train_aug_x.shape, mfcc_train_aug_y.shape)\n",
    "print(mfcc_train_x.shape, mfcc_train_y.shape)\n",
    "\n",
    "##  Happy 데이터 개수에 맞게 데이터 추출 (Happy, Neutral 제외한 클래스)\n",
    "train_anno = annotation.loc[annotation.fold=='train', :]\n",
    "happy_num = len(train_anno.loc[train_anno.emotion=='happy'])\n",
    "aug_number_happy = dict() # 감정별 추가할 데이터 개수\n",
    "for e_id in train_anno.emotion_id.unique():\n",
    "    if (e_id != 3) & (e_id != 4): # happy\n",
    "        emotion_number = len(train_anno.loc[train_anno.emotion_id==e_id])\n",
    "        aug_number_happy[e_id] = happy_num-emotion_number\n",
    "#print(aug_number_happy)\n",
    "\n",
    "# 랜덤하게 추출 (비복원추출)\n",
    "aug_happy_x = []\n",
    "aug_happy_y = []\n",
    "for e in list(aug_number_happy.keys()):\n",
    "    emotion_index = np.random.choice(np.where(mfcc_train_aug_y.numpy()==e)[0], aug_number_happy[e], replace=False)\n",
    "    aug_happy_x += mfcc_train_aug_x[emotion_index]\n",
    "    aug_happy_y += list(itertools.repeat(e, aug_number_happy[e]))\n",
    "    \n",
    "aug_happy_x = torch.stack(aug_happy_x)\n",
    "aug_happy_y = torch.LongTensor(aug_happy_y)\n",
    "\n",
    "# concat (original + augmentation data)\n",
    "aug_x_h = torch.cat((mfcc_train_x, aug_happy_x), dim=0)\n",
    "aug_y_h = torch.cat((mfcc_train_y, aug_happy_y), dim=0)\n",
    "print(aug_x_h.shape, aug_y_h.shape)\n",
    "\n",
    "# save (오디오 데이터 전처리 완료)\n",
    "torch.save((aug_x_h, aug_y_h), save_PATH+'/train_mfcc_happy_14612.pt')\n",
    "torch.save((mfcc_train_x, mfcc_train_y), save_PATH+'/train_mfcc.pt')\n",
    "\n",
    "# test data save\n",
    "mfcc_test_x, mfcc_test_y = dl.get_data(method='mfcc', fold='test')\n",
    "#print(mfcc_test_x.shape, mfcc_test_y.shape)\n",
    "torch.save((mfcc_test_x, mfcc_test_y), save_PATH+'/test_mfcc.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ee244",
   "metadata": {},
   "source": [
    "### 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a5032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (MFCC)\n",
    "train_x, train_y = torch.load(save_PATH+'/train_mfcc_happy_14612.pt') # 증강 데이터\n",
    "#train_origin_x, train_origin_y = torch.load('save_PATH+'/train_mfcc.pt') # 원본 train (성능 확인에 사용)\n",
    "test_x, test_y = torch.load(save_PATH+'/test_mfcc.pt') \n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "classifier = wavNet(in_channels=64, num_classes=7, num_features=128).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score normalization\n",
    "train_mu = train_x.mean()\n",
    "train_std = train_x.std()\n",
    "train_x = (train_x - train_mu) / train_std\n",
    "#train_origin_x = (train_origin_x - train_mu) / train_std\n",
    "test_x = (test_x - train_mu) / train_std\n",
    "\n",
    "train_avg = TensorDataset(train_x, train_y)\n",
    "\n",
    "train_len = int(len(train_x)*0.9)\n",
    "train, valid = random_split(train_avg, [train_len, len(train_x)-train_len])\n",
    "print('Train set:', len(train), 'Validation set:', len(valid))\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True) \n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  classifier.train()\n",
    "  running_loss = 0.0\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    torch.save(classifier, save_PATH+'/wav_classifier.pt')\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # gradient initialize\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = classifier(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # compute gradients of each params\n",
    "    loss.backward()\n",
    "    # optimize\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    prediction = torch.max(outputs, 1)[1]\n",
    "    correct_prediction += (prediction == y).sum()\n",
    "    total_prediction += prediction.shape[0]\n",
    "\n",
    "  num_batches = len(train_loader)\n",
    "  avg_loss = running_loss / num_batches\n",
    "  acc = correct_prediction / total_prediction\n",
    "\n",
    "  # evaluation using validation data\n",
    "  classifier.eval()\n",
    "  with torch.no_grad():\n",
    "    if (epoch == 0) | ((epoch+1) % 10 == 0):\n",
    "      val_loss, val_f1 = valid_model(classifier, valid, batch_size, criterion, device)\n",
    "      print(f'\\nEpoch: {epoch+1}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}, Val_Loss: {val_loss:.2f}, Val_F1 score: {val_f1:.2f}')\n",
    "print('Finished Training')\n",
    "\n",
    "end = time.time()\n",
    "spen_time = end - start\n",
    "result = str(datetime.timedelta(seconds=spen_time)).split(\".\")[0]            \n",
    "print('Time:', result)\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "  confmat = ConfusionMatrix(task='multiclass', num_classes=7).to(device)\n",
    "  outputs = test_model(classifier, test_x, test_y, batch_size, device)\n",
    "  \n",
    "  conf = confmat(outputs.to(device), test_y.to(device))\n",
    "  all_metrics = metrics.classification_report(torch.argmax(outputs, 1).cpu(), test_y.cpu())\n",
    "  \n",
    "  display(conf)\n",
    "  print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output \n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "print('--------[Train set]--------')\n",
    "train_anno = pd.read_pickle(PATH+'/new/annotation/train_origin.pkl')\n",
    "f1, conf = save_modelFile(path=None,\n",
    "                          x=train_x, y=train_y, model=classifier, batch_size=batch_size,\n",
    "                          annotation=train_anno, fold='train')\n",
    "print(f1)\n",
    "display(conf)\n",
    "\n",
    "print('--------[Test set]--------')\n",
    "test_anno = pd.read_pickle(PATH+'/new/annotation/test_origin.pkl')\n",
    "f1, conf = save_modelFile(path=save_PATH+'/wav_result',\n",
    "                          x=test_x, y=test_y, model=classifier, batch_size=batch_size,\n",
    "                          annotation=test_anno, fold='test')\n",
    "print(f1)\n",
    "display(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c427a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
