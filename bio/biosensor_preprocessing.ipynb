{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BIO Sensor 데이터 초기 전처리"
      ],
      "metadata": {
        "id": "JMnGDqueKJAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 Sesstion에서 script별로 나눠져있는 바이오 데이터 합치기"
      ],
      "metadata": {
        "id": "cFjkc9FiKW5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 불러올 파일 경로\n",
        "path_dir = './dataset/KEMDy20_v1_1/EDA'\n",
        "# 지정된 경로의 파일 모두 리스트에 불러오기\n",
        "sess_FList = os.listdir(path_dir)\n",
        "\n",
        "# 저장할 파일 경로\n",
        "drop_dir ='./dataset/KEMDy20_v1_1/sensor/EDA/'\n",
        "\n",
        "# 세션에서 스크립트별로 나눠져있던 eda 데이터 합치기기\n",
        "for folder in sess_FList:\n",
        "  lst = []\n",
        "  sess_save=[] # ('session_number', 'EDA')\n",
        "  eda_FList = os.listdir(path_dir+'/'+folder)\n",
        "  for script in eda_FList:\n",
        "      with open('./dataset/KEMDy20_v1_1/EDA/'+folder+'/'+script) as lines:\n",
        "        b = list(map(str, script.split('_')))\n",
        "        for line in lines:\n",
        "          data = line.strip().split(',')\n",
        "          if len(data)==3:\n",
        "            a = list(map(str, data[2].split('_')))\n",
        "            data[2] = data[2].replace(a[0],b[0])\n",
        "            lst.append(data)\n",
        "  df = pd.DataFrame(lst, columns=['data','date','sess'])\n",
        "  df = df.astype({'data':float})  \n",
        "  \n",
        "  sess_name = df.sess.unique()\n",
        "  for sess in tqdm(sess_name):\n",
        "    eda=list(df[df.sess == sess].data.values)\n",
        "    sess_list = [sess, eda]\n",
        "    sess_save.append(sess_list)\n",
        "  sess_save_df = pd.DataFrame(sess_save, columns=['session', 'eda'])\n",
        "  sess_save_df.to_pickle(drop_dir+folder+'_eda.pkl')"
      ],
      "metadata": {
        "id": "pMnf80DXKExm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 불러올 파일 경로\n",
        "path_dir = './dataset/KEMDy20_v1_1/TEMP'\n",
        "# 지정된 경로의 파일 모두 리스트에 불러오기\n",
        "sess_FList = os.listdir(path_dir)\n",
        "\n",
        "# 저장할 파일 경로\n",
        "drop_dir ='./dataset/KEMDy20_v1_1/sensor/Temp/'\n",
        "\n",
        "# 세션에서 스크립트별로 나눠져있던 temp 데이터 합치기\n",
        "for folder in sess_FList:\n",
        "  lst = []\n",
        "  sess_save=[] \n",
        "  temp_FList = os.listdir(path_dir+'/'+folder)\n",
        "  for script in temp_FList:\n",
        "      with open('./dataset/KEMDy20_v1_1/TEMP/'+folder+'/'+script) as lines:\n",
        "        b = list(map(str, script.split('_')))\n",
        "        for line in lines:\n",
        "          data = line.strip().split(',')\n",
        "          if len(data)==3:\n",
        "            a = list(map(str, data[2].split('_')))\n",
        "            data[2] = data[2].replace(a[0],b[0])\n",
        "            lst.append(data)\n",
        "  df = pd.DataFrame(lst, columns=['data','date','sess'])\n",
        "  df = df.astype({'data':float})  \n",
        "  \n",
        "  sess_name = df.sess.unique()\n",
        "  for sess in tqdm(sess_name):\n",
        "    eda=list(df[df.sess == sess].data.values)\n",
        "    sess_list = [sess, eda]\n",
        "    sess_save.append(sess_list)\n",
        "\n",
        "  sess_save_df = pd.DataFrame(sess_save, columns=['session', 'temp'])\n",
        "  sess_save_df.to_pickle(drop_dir+folder+'_temp.pkl')"
      ],
      "metadata": {
        "id": "iCGLCgbPKUSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 바이오 데이터 합치기"
      ],
      "metadata": {
        "id": "Le-bf-kDKgEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# 세션별 저장한 바이오(eda, temo)파일 경로 지정\n",
        "eda_dir = './dataset/KEMDy20_v1_1/sensor/EDA'\n",
        "temp_dir = './dataset/KEMDy20_v1_1/sensor/Temp'\n",
        "\n",
        "# 지젇된 경로 파일 불러오기 및 정렬렬\n",
        "eda_sess_FList = sorted(os.listdir(eda_dir))\n",
        "temp_sess_FList = sorted(os.listdir(temp_dir))\n",
        "\n",
        "# 빈 데이터 프레임 만들기\n",
        "bio_all = pd.DataFrame(columns=['segment_id','eda','temp'])\n",
        "\n",
        "# 세션별 저장된 바이오(eda,temp) 데이터 합치기\n",
        "for eda, temp in zip(eda_sess_FList, temp_sess_FList):\n",
        "  print(eda, temp)\n",
        "  TEMP = pd.read_pickle('./dataset/KEMDy20_v1_1/sensor/Temp/'+ temp )\n",
        "  EDA = pd.read_pickle('./dataset//KEMDy20_v1_1/sensor/EDA/'+ eda )\n",
        "  TEMP.columns = ['segment_id','temp']\n",
        "  EDA.columns = ['segment_id','eda']\n",
        "  bio = EDA.merge(TEMP, on ='segment_id')\n",
        "  bio_all = pd.concat([bio_all,bio] ignore_index=True)\n",
        "\n",
        "  bio_all"
      ],
      "metadata": {
        "id": "mK2pfv02KgN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "segment_id를 기준으로 annotation데이터와 합치기"
      ],
      "metadata": {
        "id": "Xu7W9JZagGsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리된 annotation 데이터 불러오기\n",
        "\n",
        "train = pd.read_pickle('./dataset/KEMDy20_v1_1/new/annotation/train_origin.pkl')\n",
        "test = pd.read_pickle('./dataset/KEMDy20_v1_1/new/annotation/test_origin.pkl')"
      ],
      "metadata": {
        "id": "thwSIttKgAOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# annotation 데이터와 segment_id 기준으로 merge\n",
        "\n",
        "bio_train = train.merge(bio_all, how = 'inner', on = 'segment_id')\n",
        "bio_test = test.merge(bio_all, how = 'inner', on = 'segment_id')"
      ],
      "metadata": {
        "id": "0CAbprpilg1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종종 바이오 데이터 프레임 저장\n",
        "\n",
        "bio_train.to_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/train_origin.pkl')\n",
        "bio_test.to_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/test_origin.pkl')"
      ],
      "metadata": {
        "id": "lLzh4R6rlg33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 증강 및 패딩 과정"
      ],
      "metadata": {
        "id": "wYTvVFgAhBA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "train = pd.read_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/train_origin.pkl') \n",
        "test = pd.read_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/test_origin.pkl')"
      ],
      "metadata": {
        "id": "fZhdapMzCmEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 함수\n",
        "def get_numpy_from_nonfixed_2d_array(aa, fixed_length=141):\n",
        "    rows = []\n",
        "    for a in aa:\n",
        "        rows.append(np.pad(a, (0, fixed_length), 'constant', constant_values=0)[:fixed_length])\n",
        "    return np.concatenate(rows, axis=0).reshape(-1, fixed_length)"
      ],
      "metadata": {
        "id": "eLk6Wuf_g-oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 패딩 진행\n",
        "temp = get_numpy_from_nonfixed_2d_array(train['temp'])\n",
        "eda = get_numpy_from_nonfixed_2d_array(train['eda'])\n",
        "train_y = train.emotion_id.values"
      ],
      "metadata": {
        "id": "oZ8SchhrCbqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"기쁨\" 맞춰 데이터 증강"
      ],
      "metadata": {
        "id": "cc4znl8JBBpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Temp=pd.DataFrame(temp)\n",
        "Temp['emotion']=pd.DataFrame(train_y)\n",
        "\n",
        "EDA = pd.DataFrame(eda)\n",
        "EDA['emotion']=pd.DataFrame(train_y)\n",
        "\n",
        "nedf = EDA.loc[(EDA.emotion==4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "ntdf = Temp.loc[(Temp.emotion==4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "n_y = Temp.loc[(Temp.emotion==4),'emotion'].to_numpy()\n",
        "\n",
        "edf = EDA.loc[(EDA.emotion!=4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "tdf = Temp.loc[(Temp.emotion!=4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "y =EDA.loc[(EDA.emotion!=4),'emotion'].to_numpy()"
      ],
      "metadata": {
        "id": "QA3fFONsAu7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state= 1024)\n",
        "temp_x,t_y = smote.fit_resample(tdf,y)\n",
        "eda_x,t_y = smote.fit_resample(edf,y)"
      ],
      "metadata": {
        "id": "P0zJLC4XA-7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "htemp = np.concatenate((temp_x,ntdf), axis=0)\n",
        "heda = np.concatenate((eda_x,nedf), axis=0)\n",
        "h_y = np.concatenate((t_y,n_y), axis=0)"
      ],
      "metadata": {
        "id": "th-6MCCEBBwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# annotation과 합친 바이오데이터 파일 불러오기기\n",
        "train = pd.read_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/train_origin.pkl')\n",
        "test = pd.read_pickle('./dataset/KEMDy20_v1_1/sensor/bio_train/test_origin.pkl')\n",
        "\n",
        "# 데이터 padding\n",
        "temp = get_numpy_from_nonfixed_2d_array(train['temp'])\n",
        "eda = get_numpy_from_nonfixed_2d_array(train['eda'])\n",
        "train_y = train.emotion_id.values\n",
        "\n",
        "# padding한 array에서 \"중립\" 클래스 데이터 제외\n",
        "Temp=pd.DataFrame(temp)\n",
        "Temp['emotion']=pd.DataFrame(train_y)\n",
        "\n",
        "EDA = pd.DataFrame(eda)\n",
        "EDA['emotion']=pd.DataFrame(train_y)\n",
        "\n",
        "nedf = EDA.loc[(EDA.emotion==4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "ntdf = Temp.loc[(Temp.emotion==4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "n_y = Temp.loc[(Temp.emotion==4),'emotion'].to_numpy()\n",
        "\n",
        "edf = EDA.loc[(EDA.emotion!=4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "tdf = Temp.loc[(Temp.emotion!=4),:].drop(['emotion'],axis=1).to_numpy()\n",
        "y =EDA.loc[(EDA.emotion!=4),'emotion'].to_numpy()"
      ],
      "metadata": {
        "id": "1fIbWIAvhIOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 증강\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state= 1024)\n",
        "temp_x,t_y = smote.fit_resample(tdf,y)\n",
        "eda_x,t_y = smote.fit_resample(edf,y)"
      ],
      "metadata": {
        "id": "zCT5yHhTB0Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"중립\" 클래스 데이터와 합치기기\n",
        "htemp = np.concatenate((temp_x,ntdf), axis=0)\n",
        "heda = np.concatenate((eda_x,nedf), axis=0)\n",
        "h_y = np.concatenate((t_y,n_y), axis=0)"
      ],
      "metadata": {
        "id": "wZQIdDtSB2Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 증강한 바이오 데이터 배열 저장\n",
        "np.save('./dataset/KEMDy20_v1_1/sensor/bio_train/temp_x.npy',htemp)\n",
        "np.save('./dataset/KEMDy20_v1_1/sensor/bio_train/eda_x.npy',heda)\n",
        "np.save('./dataset/KEMDy20_v1_1/sensor/bio_train/t_y.npy',h_y)"
      ],
      "metadata": {
        "id": "NSBDUdIihp9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
